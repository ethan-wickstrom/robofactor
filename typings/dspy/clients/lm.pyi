"""
This type stub file was generated by pyright.
"""

from typing import Any, Dict, List, Literal, Optional
from dspy.clients.provider import Provider, ReinforceJob, TrainingJob
from dspy.clients.utils_finetune import TrainDataFormat
from dspy.utils.callback import BaseCallback
from .base_lm import BaseLM

logger = ...

class LM(BaseLM):
    """
    A language model supporting chat or text completion requests for use with DSPy modules.
    """
    def __init__(
        self,
        model: str,
        model_type: Literal["chat", "text"] = ...,
        temperature: float = ...,
        max_tokens: int = ...,
        cache: bool = ...,
        cache_in_memory: bool = ...,
        callbacks: Optional[List[BaseCallback]] = ...,
        num_retries: int = ...,
        provider=...,
        finetuning_model: Optional[str] = ...,
        launch_kwargs: Optional[dict[str, Any]] = ...,
        train_kwargs: Optional[dict[str, Any]] = ...,
        **kwargs,
    ) -> None:
        """
        Create a new language model instance for use with DSPy modules and programs.

        Args:
            model: The model to use. This should be a string of the form ``"llm_provider/llm_name"``
                   supported by LiteLLM. For example, ``"openai/gpt-4o"``.
            model_type: The type of the model, either ``"chat"`` or ``"text"``.
            temperature: The sampling temperature to use when generating responses.
            max_tokens: The maximum number of tokens to generate per response.
            cache: Whether to cache the model responses for reuse to improve performance
                   and reduce costs.
            cache_in_memory (deprecated): To enable additional caching with LRU in memory.
            callbacks: A list of callback functions to run before and after each request.
            num_retries: The number of times to retry a request if it fails transiently due to
                         network error, rate limiting, etc. Requests are retried with exponential
                         backoff.
            provider: The provider to use. If not specified, the provider will be inferred from the model.
            finetuning_model: The model to finetune. In some providers, the models available for finetuning is different
                from the models available for inference.
        """
        ...

    def forward(self, prompt=..., messages=..., **kwargs):  # -> Any | CoroutineType[Any, Any, Any]:
        ...
    async def aforward(self, prompt=..., messages=..., **kwargs):  # -> Any:
        ...
    def launch(self, launch_kwargs: Optional[Dict[str, Any]] = ...):  # -> None:
        ...
    def kill(self, launch_kwargs: Optional[Dict[str, Any]] = ...):  # -> None:
        ...
    def finetune(
        self,
        train_data: List[Dict[str, Any]],
        train_data_format: Optional[TrainDataFormat],
        train_kwargs: Optional[Dict[str, Any]] = ...,
    ) -> TrainingJob: ...
    def reinforce(self, train_kwargs) -> ReinforceJob: ...
    def infer_provider(self) -> Provider: ...
    def dump_state(self):  # -> dict[str, Any]:
        ...

def litellm_completion(
    request: Dict[str, Any], num_retries: int, cache: Optional[Dict[str, Any]] = ...
):  # -> ModelResponse | CustomStreamWrapper | TextCompletionResponse | CoroutineType[Any, Any, ModelResponse | TextCompletionResponse | None] | None:
    ...
def litellm_text_completion(
    request: Dict[str, Any], num_retries: int, cache: Optional[Dict[str, Any]] = ...
):  # -> TextCompletionResponse | ModelResponse | CustomStreamWrapper | TextCompletionStreamWrapper | <subclass of ModelResponse and TextCompletionStreamWrapper> | <subclass of ModelResponse and TextCompletionResponse>:
    ...
async def alitellm_completion(
    request: Dict[str, Any], num_retries: int, cache: Optional[Dict[str, Any]] = ...
):  # -> ModelResponse | CustomStreamWrapper | TextCompletionResponse | None:
    ...
async def alitellm_text_completion(
    request: Dict[str, Any], num_retries: int, cache: Optional[Dict[str, Any]] = ...
):  # -> TextCompletionResponse | TextCompletionStreamWrapper:
    ...
