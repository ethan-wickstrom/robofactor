"""
This type stub file was generated by pyright.
"""

import pandas as pd
import dspy
from typing import Any, Callable, List, Optional, TYPE_CHECKING, Union
from dspy.utils.callback import with_callbacks

if TYPE_CHECKING: ...
logger = ...

class Evaluate:
    """DSPy Evaluate class.

    This class is used to evaluate the performance of a DSPy program. Users need to provide a evaluation dataset and
    a metric function in order to use this class. This class supports parallel evaluation on the provided dataset.
    """
    def __init__(
        self,
        *,
        devset: List[dspy.Example],
        metric: Optional[Callable] = ...,
        num_threads: Optional[int] = ...,
        display_progress: bool = ...,
        display_table: Union[bool, int] = ...,
        max_errors: int = ...,
        return_all_scores: bool = ...,
        return_outputs: bool = ...,
        provide_traceback: Optional[bool] = ...,
        failure_score: float = ...,
        **kwargs,
    ) -> None:
        """
        Args:
            devset (List[dspy.Example]): the evaluation dataset.
            metric (Callable): The metric function to use for evaluation.
            num_threads (Optional[int]): The number of threads to use for parallel evaluation.
            display_progress (bool): Whether to display progress during evaluation.
            display_table (Union[bool, int]): Whether to display the evaluation results in a table.
                If a number is passed, the evaluation results will be truncated to that number before displayed.
            max_errors (int): The maximum number of errors to allow before stopping evaluation.
            return_all_scores (bool): Whether to return scores for every data record in `devset`.
            return_outputs (bool): Whether to return the dspy program's outputs for every data in `devset`.
            provide_traceback (Optional[bool]): Whether to provide traceback information during evaluation.
            failure_score (float): The default score to use if evaluation fails due to an exception.
        """
        ...

    @with_callbacks
    def __call__(
        self,
        program: dspy.Module,
        metric: Optional[Callable] = ...,
        devset: Optional[List[dspy.Example]] = ...,
        num_threads: Optional[int] = ...,
        display_progress: Optional[bool] = ...,
        display_table: Optional[Union[bool, int]] = ...,
        return_all_scores: Optional[bool] = ...,
        return_outputs: Optional[bool] = ...,
        callback_metadata: Optional[dict[str, Any]] = ...,
    ):  # -> tuple[float, list[tuple[Example, Prediction, float]], list[float]] | tuple[float, list[float]] | tuple[float, list[tuple[Example, Prediction, float]]] | float:
        """
        Args:
            program (dspy.Module): The DSPy program to evaluate.
            metric (Callable): The metric function to use for evaluation. if not provided, use `self.metric`.
            devset (List[dspy.Example]): the evaluation dataset. if not provided, use `self.devset`.
            num_threads (Optional[int]): The number of threads to use for parallel evaluation. if not provided, use
                `self.num_threads`.
            display_progress (bool): Whether to display progress during evaluation. if not provided, use
                `self.display_progress`.
            display_table (Union[bool, int]): Whether to display the evaluation results in a table. if not provided, use
                `self.display_table`. If a number is passed, the evaluation results will be truncated to that number before displayed.
            return_all_scores (bool): Whether to return scores for every data record in `devset`. if not provided,
                use `self.return_all_scores`.
            return_outputs (bool): Whether to return the dspy program's outputs for every data in `devset`. if not
                provided, use `self.return_outputs`.
            callback_metadata (dict): Metadata to be used for evaluate callback handlers.

        Returns:
            The evaluation results are returned in different formats based on the flags:

            - Base return: A float percentage score (e.g., 67.30) representing overall performance

            - With `return_all_scores=True`:
                Returns (overall_score, individual_scores) where individual_scores is a list of
                float scores for each example in devset

            - With `return_outputs=True`:
                Returns (overall_score, result_triples) where result_triples is a list of
                (example, prediction, score) tuples for each example in devset

            - With both flags=True:
                Returns (overall_score, result_triples, individual_scores)

        """
        ...

def prediction_is_dictlike(prediction):  # -> TypeIs[Callable[..., object]] | Literal[False]:
    ...
def merge_dicts(d1, d2) -> dict: ...
def truncate_cell(content) -> str:
    """Truncate content of a cell to 25 words."""
    ...

def stylize_metric_name(df: pd.DataFrame, metric_name: str) -> pd.DataFrame:
    """
    Stylize the cell contents of a pandas DataFrame corresponding to the specified metric name.

    :param df: The pandas DataFrame for which to stylize cell contents.
    :param metric_name: The name of the metric for which to stylize DataFrame cell contents.
    """
    ...

def display_dataframe(df: pd.DataFrame):  # -> None:
    """
    Display the specified Pandas DataFrame in the console.

    :param df: The Pandas DataFrame to display.
    """
    ...

def configure_dataframe_for_ipython_notebook_display(df: pd.DataFrame) -> pd.DataFrame:
    """Set various pandas display options for DataFrame in an IPython notebook environment."""
    ...

def is_in_ipython_notebook_environment():  # -> bool:
    """
    Check if the current environment is an IPython notebook environment.

    :return: True if the current environment is an IPython notebook environment, False otherwise.
    """
    ...
