"""
This type stub file was generated by pyright.
"""

logger = ...
def create_minibatch(trainset, batch_size=..., rng=...): # -> list[Any]:
    """Create a minibatch from the trainset."""
    ...

def eval_candidate_program(batch_size, trainset, candidate_program, evaluate, rng=...): # -> Prediction:
    """Evaluate a candidate program on the trainset, using the specified batch size."""
    ...

def eval_candidate_program_with_pruning(trial, trial_logs, trainset, candidate_program, evaluate, trial_num, batch_size=...): # -> tuple[Any, Any, int, Literal[True]] | tuple[Any, Any, int, Literal[False]]:
    """Evaluation of candidate_program with pruning implemented"""
    ...

def get_program_with_highest_avg_score(param_score_dict, fully_evaled_param_combos): # -> tuple[Any, Any, Any, Any] | tuple[Any, Any | floating[Any], Any, Any]:
    """Used as a helper function for bayesian + minibatching optimizers. Returns the program with the highest average score from the batches evaluated so far."""
    ...

def calculate_last_n_proposed_quality(base_program, trial_logs, evaluate, trainset, devset, n): # -> tuple[Any | Literal[0], Any, Any | Literal[0], Any]:
    """
    Calculate the average and best quality of the last n programs proposed. This is useful for seeing if our proposals
    are actually 'improving' overtime or not.
    """
    ...

def get_task_model_history_for_full_example(candidate_program, task_model, devset, evaluate):
    """Get a full trace of the task model's history for a given candidate program."""
    ...

def print_full_program(program): # -> None:
    """Print out the program's instructions & prefixes for each module."""
    ...

def save_candidate_program(program, log_dir, trial_num, note=...): # -> str | None:
    """Save the candidate program to the log directory."""
    ...

def save_file_to_log_dir(source_file_path, log_dir): # -> None:
    ...

def setup_logging(log_dir): # -> None:
    """Setup logger, which will log our print statements to a txt file at our log_dir for later viewing"""
    ...

def get_token_usage(model) -> tuple[int, int]:
    """
    Extract total input tokens and output tokens from a model's interaction history.
    Returns (total_input_tokens, total_output_tokens).
    """
    ...

def log_token_usage(trial_logs, trial_num, model_dict): # -> None:
    """
    Extract total input and output tokens used by each model and log to trial_logs[trial_num]["token_usage"].
    """
    ...

def get_prompt_model(prompt_model):
    ...

def get_signature(predictor):
    ...

def set_signature(predictor, updated_signature): # -> None:
    ...

def create_n_fewshot_demo_sets(student, num_candidate_sets, trainset, max_labeled_demos, max_bootstrapped_demos, metric, teacher_settings, max_errors=..., max_rounds=..., labeled_sample=..., min_num_samples=..., metric_threshold=..., teacher=..., include_non_bootstrapped=..., seed=..., rng=...): # -> dict[Any, Any]:
    """
    This function is copied from random_search.py, and creates fewshot examples in the same way that random search does.
    This allows us to take advantage of using the same fewshot examples when we use the same random seed in our optimizers.
    """
    ...

def old_getfile(object): # -> str | None:
    """Work out which source or compiled file an object was defined in."""
    ...

def new_getfile(object): # -> str | None:
    ...

